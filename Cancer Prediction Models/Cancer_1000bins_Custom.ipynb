{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c29ad23-5520-400b-aa40-b6250e3ed579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data normalization and preparation completed.\n",
      "Features shape: (517, 1000), Labels shape: (517,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Paths to data\n",
    "data_folder = r\"C:\\Users\\sejac\\Desktop\\csv_labels\"\n",
    "top_bins_path = r\"C:\\Users\\sejac\\Desktop\\top_1000_bins_u_test.csv\"\n",
    "\n",
    "# Function to calculate z-scores with special handling for zero entries\n",
    "def calculate_z_scores(data):\n",
    "    # Exclude zeros when calculating mean and standard deviation\n",
    "    non_zero_data = data[data != 0]\n",
    "    mean = non_zero_data.mean()\n",
    "    std = non_zero_data.std()\n",
    "\n",
    "    # Avoid division by zero in case all non-zero values are the same\n",
    "    if std == 0:\n",
    "        return np.zeros_like(data)  # Return all zeros if standard deviation is zero\n",
    "\n",
    "    # Calculate z-scores for non-zero values\n",
    "    z_scores = (data - mean) / std\n",
    "    z_scores[data == 0] = 0  # Keep zeros as zero\n",
    "    return z_scores\n",
    "\n",
    "# Load top 1000 bins\n",
    "top_bins = pd.read_csv(top_bins_path)['Bin_ID']\n",
    "\n",
    "# Initialize list to store all patients' data\n",
    "all_patients_data = []\n",
    "\n",
    "# Loop through all patient files\n",
    "for filename in os.listdir(data_folder):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        # Determine cancer label from filename\n",
    "        if \"_0_\" in filename:\n",
    "            label = 0  # Non-cancer\n",
    "        elif \"_1_\" in filename:\n",
    "            label = 1  # Cancer\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected filename format: {filename}\")\n",
    "        \n",
    "        # Load patient data\n",
    "        file_path = os.path.join(data_folder, filename)\n",
    "        patient_data = pd.read_csv(file_path)\n",
    "        \n",
    "        # Create unique Bin_ID (Chromosome + Start)\n",
    "        patient_data['Bin_ID'] = patient_data['Chromosome'] + \":\" + patient_data['Start'].astype(str)\n",
    "        \n",
    "        # Filter for top bins\n",
    "        patient_data = patient_data[patient_data['Bin_ID'].isin(top_bins)]\n",
    "        \n",
    "        # Ensure consistent data length (after filtering for top bins)\n",
    "        if len(patient_data) != len(top_bins):\n",
    "            print(f\"Inconsistent data length for {filename}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Normalize Percent_Methylation using z-scores\n",
    "        normalized_methylation = calculate_z_scores(patient_data['Percent_Methylation'].values)\n",
    "        patient_data['Percent_Methylation'] = normalized_methylation\n",
    "        \n",
    "        # Set Bin_ID as the index for easier merging later\n",
    "        patient_data = patient_data.set_index('Bin_ID')['Percent_Methylation']\n",
    "        \n",
    "        # Add label and patient identifier\n",
    "        patient_data = patient_data.to_frame(name=filename).T  # Transpose for merging\n",
    "        patient_data['Label'] = label  # Add label column\n",
    "        \n",
    "        # Append to list\n",
    "        all_patients_data.append(patient_data)\n",
    "\n",
    "# Combine all patient data into a single DataFrame\n",
    "final_data = pd.concat(all_patients_data).reset_index(drop=True)\n",
    "\n",
    "# Extract labels\n",
    "labels = final_data['Label']\n",
    "features = final_data.drop('Label', axis=1)\n",
    "\n",
    "# Fill missing values with column mean (optional, but unlikely needed after z-normalization)\n",
    "features.fillna(features.mean(), inplace=True)\n",
    "\n",
    "print(\"Data normalization and preparation completed.\")\n",
    "print(f\"Features shape: {features.shape}, Labels shape: {labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b9fe6c9-c42d-4ed5-a928-7ee5149af0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (517, 1000)\n",
      "Labels shape: (517,)\n",
      "Sample labels: 0    0\n",
      "1    0\n",
      "2    0\n",
      "3    1\n",
      "4    0\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"Features shape: {features.shape}\")  # Should be (num_patients, 1000)\n",
    "print(f\"Labels shape: {labels.shape}\")      # Should match the number of patients\n",
    "print(f\"Sample labels: {labels.head()}\")    # Verify label values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a8db975-4044-49eb-bd6d-9f34d6690708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SparseConnectionLayer(nn.Module):\n",
    "    def __init__(self, input_size, output_size, connections_per_output):\n",
    "        \"\"\"\n",
    "        Custom sparse layer with localized sparse connections.\n",
    "        Args:\n",
    "            input_size: Total number of neurons in the input layer.\n",
    "            output_size: Total number of neurons in the output layer.\n",
    "            connections_per_output: Number of input neurons connected to each output neuron.\n",
    "        \"\"\"\n",
    "        super(SparseConnectionLayer, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.connections_per_output = connections_per_output\n",
    "\n",
    "        # Predefine sparse connectivity\n",
    "        self.connections = self._generate_connections()\n",
    "\n",
    "        # Parameters: weights and biases for each output neuron\n",
    "        self.weights = nn.Parameter(torch.randn(output_size, connections_per_output))\n",
    "        self.bias = nn.Parameter(torch.zeros(output_size))\n",
    "\n",
    "    def _generate_connections(self):\n",
    "        \"\"\"\n",
    "        Generate a fixed sparse connection pattern.\n",
    "        Returns:\n",
    "            A list of input neuron indices for each output neuron.\n",
    "        \"\"\"\n",
    "        connections = []\n",
    "        for i in range(self.output_size):\n",
    "            # Select `connections_per_output` input neurons for each output neuron\n",
    "            start_idx = (i * self.connections_per_output) % self.input_size\n",
    "            connections.append(\n",
    "                [(start_idx + j) % self.input_size for j in range(self.connections_per_output)]\n",
    "            )\n",
    "        return connections\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass with sparse connections.\n",
    "        Args:\n",
    "            x: Input tensor of shape [batch_size, input_size].\n",
    "        Returns:\n",
    "            Tensor of shape [batch_size, output_size].\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        output = []\n",
    "        for i, indices in enumerate(self.connections):\n",
    "            # Select the relevant input neurons for the current output neuron\n",
    "            input_subset = x[:, indices]  # Shape: [batch_size, connections_per_output]\n",
    "\n",
    "            # Compute weighted sum for this sparse connection\n",
    "            weighted_sum = torch.matmul(input_subset, self.weights[i]) + self.bias[i]\n",
    "            output.append(weighted_sum)\n",
    "\n",
    "        return torch.stack(output, dim=1)  # Shape: [batch_size, output_size]\n",
    "\n",
    "\n",
    "class CancerPredictionModelSparse(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CancerPredictionModelSparse, self).__init__()\n",
    "        # Define sparse layers for 1000 bins\n",
    "        self.hidden1 = SparseConnectionLayer(input_size=1000, output_size=300, connections_per_output=3)\n",
    "        self.hidden2 = SparseConnectionLayer(input_size=300, output_size=100, connections_per_output=3)\n",
    "        self.hidden3 = SparseConnectionLayer(input_size=100, output_size=50, connections_per_output=3)\n",
    "        self.hidden4 = SparseConnectionLayer(input_size=50, output_size=20, connections_per_output=3)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(20, 10)  # Fully connected layer (20 -> 10)\n",
    "        self.fc2 = nn.Linear(10, 5)   # Fully connected layer (10 -> 5)\n",
    "        self.fc3 = nn.Linear(5, 1)    # Fully connected layer (5 -> 1)\n",
    "\n",
    "        # Activation functions\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through sparse layers\n",
    "        x = self.hidden1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.hidden2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.hidden3(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.hidden4(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # Pass through fully connected layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        # Final sigmoid activation for binary classification\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c2e353a-b25b-4574-8a4d-7d85d259d37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 330\n",
      "Validation data size: 83\n",
      "Test data size: 104\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert features and labels to NumPy\n",
    "X = features.values  # Shape: (num_patients, 1000)\n",
    "y = labels.values    # Shape: (num_patients,)\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)  # Float for BCE loss\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Training data size: {len(train_loader.dataset)}\")\n",
    "print(f\"Validation data size: {len(val_loader.dataset)}\")\n",
    "print(f\"Test data size: {len(test_loader.dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b9d36d-8ec3-42c7-a0c3-e446036fe116",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
